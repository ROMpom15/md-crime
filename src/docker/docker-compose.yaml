# Base file was developed from Brett Gentile's SD411 lab04

services:
   namenode:
      image: apache/hadoop:3
      hostname: namenode
      command: ["hdfs", "namenode"]
      ports:
        - 9870:9870
      env_file:
        - ./config
      environment:
          ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
      volumes: # Recommended by Gemini
        - ./data:/local_data_mount             # Access local files directly
        - namenode_data:/tmp/hadoop-root/dfs/name # Save HDFS state

   datanode:
      image: apache/hadoop:3
      command: ["hdfs", "datanode"]
      env_file:
        - ./config  
      volumes:
        - datanode_data:/tmp/hadoop-root/dfs/data # Save HDFS blocks, Gemini rec

   resourcemanager:
      image: apache/hadoop:3
      hostname: resourcemanager
      command: ["yarn", "resourcemanager"]
      ports:
         - 8088:8088
      env_file:
        - ./config
      volumes:
        - ./test.sh:/opt/test.sh

   nodemanager:
      image: apache/hadoop:3
      command: ["yarn", "nodemanager"]
      env_file:
        - ./config

  # --- SPARK SERVICES --- All portions of this yaml file were generated by Gemini.
   spark-master:
      image: bitnami/spark:3
      hostname: spark-master
      environment:
        - SPARK_MODE=master
        - SPARK_RPC_AUTHENTICATION_ENABLED=no
        - SPARK_RPC_ENCRYPTION_ENABLED=no
        - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
        - SPARK_SSL_ENABLED=no
        # Tell Spark where HDFS is (matches your dfs.namenode.rpc-address)
        - SPARK_HADOOP_fs_defaultFS=hdfs://namenode:8020
        # ensure Spark uses the correct user to avoid permission issues
        - SPARK_HADOOP_hadoop_user_name=root 
      ports:
        - "8080:8080"
        - "7077:7077"

   spark-worker:
      image: bitnami/spark:3
      environment:
        - SPARK_MODE=worker
        - SPARK_MASTER_URL=spark://spark-master:7077
        - SPARK_WORKER_MEMORY=1G
        - SPARK_WORKER_CORES=1
        - SPARK_RPC_AUTHENTICATION_ENABLED=no
        - SPARK_RPC_ENCRYPTION_ENABLED=no
        - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
        - SPARK_SSL_ENABLED=no
        # explicitly go to port 8020
        - SPARK_HADOOP_fs_defaultFS=hdfs://namenode:8020
        - SPARK_HADOOP_hadoop_user_name=root
      depends_on:
        - spark-master
        - namenode # Wait for HDFS to be ready

volumes:
  namenode_data:
  datanode_data: